{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26eecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\data.csv\n",
      "Shape: (103000, 18)\n",
      "Attack rate: 0.02912621359223301\n",
      "\n",
      "Split sizes:\n",
      "Train full: (82400, 11) Test: (20600, 11)\n",
      "Train: (65920, 11) Val: (16480, 11)\n",
      "Train normals only: (64000, 11)\n",
      "\n",
      "Feature types:\n",
      "Numeric: 8 ['resource', 'auth_method', 'hour', 'day_of_week', 'login_success', 'new_device', 'geo_anomaly', 'privilege_escalation']\n",
      "Categorical: 3 ['geo_country', 'login_result', 'privilege_level']\n",
      "\n",
      "=== IsolationForest ===\n",
      "{'model': 'IsolationForest', 'tn': 18857, 'fp': 1143, 'fn': 15, 'tp': 585, 'accuracy': 0.9437864077669903, 'precision': 0.3385416666666667, 'recall': 0.975, 'f1': 0.5025773195876289, 'roc_auc': 0.9579930416666667, 'pr_auc': 0.2671231957993937, 'threshold': 0.06535240841251712}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9992    0.9428    0.9702     20000\n",
      "           1     0.3385    0.9750    0.5026       600\n",
      "\n",
      "    accuracy                         0.9438     20600\n",
      "   macro avg     0.6689    0.9589    0.7364     20600\n",
      "weighted avg     0.9800    0.9438    0.9566     20600\n",
      "\n",
      "\n",
      "=== OneClassSVM ===\n",
      "{'model': 'OneClassSVM', 'tn': 19971, 'fp': 29, 'fn': 183, 'tp': 417, 'accuracy': 0.9897087378640776, 'precision': 0.9349775784753364, 'recall': 0.695, 'f1': 0.7973231357552581, 'roc_auc': 0.9134064583333333, 'pr_auc': 0.7395758396986243, 'threshold': 100.61778321206526}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9986    0.9947     20000\n",
      "           1     0.9350    0.6950    0.7973       600\n",
      "\n",
      "    accuracy                         0.9897     20600\n",
      "   macro avg     0.9629    0.8468    0.8960     20600\n",
      "weighted avg     0.9893    0.9897    0.9890     20600\n",
      "\n",
      "\n",
      "=== LocalOutlierFactor ===\n",
      "{'model': 'LocalOutlierFactor', 'tn': 19867, 'fp': 133, 'fn': 183, 'tp': 417, 'accuracy': 0.9846601941747573, 'precision': 0.7581818181818182, 'recall': 0.695, 'f1': 0.7252173913043478, 'roc_auc': 0.8153544583333334, 'pr_auc': 0.6748837477755801, 'threshold': 0.007457531221793255}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9933    0.9921     20000\n",
      "           1     0.7582    0.6950    0.7252       600\n",
      "\n",
      "    accuracy                         0.9847     20600\n",
      "   macro avg     0.8745    0.8442    0.8587     20600\n",
      "weighted avg     0.9841    0.9847    0.9843     20600\n",
      "\n",
      "Epoch 001 | train_loss=0.190623 | val_loss=0.094319\n",
      "Epoch 002 | train_loss=0.086666 | val_loss=0.047042\n",
      "Epoch 003 | train_loss=0.062565 | val_loss=0.036059\n",
      "Epoch 004 | train_loss=0.053228 | val_loss=0.032349\n",
      "Epoch 005 | train_loss=0.047875 | val_loss=0.030232\n",
      "Epoch 006 | train_loss=0.043820 | val_loss=0.027191\n",
      "Epoch 007 | train_loss=0.040933 | val_loss=0.024941\n",
      "Epoch 008 | train_loss=0.038754 | val_loss=0.024348\n",
      "Epoch 009 | train_loss=0.036928 | val_loss=0.023469\n",
      "Epoch 010 | train_loss=0.035278 | val_loss=0.022773\n",
      "Epoch 011 | train_loss=0.033905 | val_loss=0.023426\n",
      "Epoch 012 | train_loss=0.032703 | val_loss=0.021805\n",
      "Epoch 013 | train_loss=0.031496 | val_loss=0.021126\n",
      "Epoch 014 | train_loss=0.030595 | val_loss=0.020904\n",
      "Epoch 015 | train_loss=0.029668 | val_loss=0.020521\n",
      "Epoch 016 | train_loss=0.029097 | val_loss=0.018823\n",
      "Epoch 017 | train_loss=0.028653 | val_loss=0.019917\n",
      "Epoch 018 | train_loss=0.027897 | val_loss=0.018173\n",
      "Epoch 019 | train_loss=0.027473 | val_loss=0.020624\n",
      "Epoch 020 | train_loss=0.026911 | val_loss=0.018787\n",
      "Epoch 021 | train_loss=0.026043 | val_loss=0.017950\n",
      "Epoch 022 | train_loss=0.025856 | val_loss=0.018144\n",
      "Epoch 023 | train_loss=0.025372 | val_loss=0.017946\n",
      "Epoch 024 | train_loss=0.024788 | val_loss=0.018276\n",
      "Epoch 025 | train_loss=0.024127 | val_loss=0.016265\n",
      "Epoch 026 | train_loss=0.023769 | val_loss=0.016139\n",
      "Epoch 027 | train_loss=0.023607 | val_loss=0.017778\n",
      "Epoch 028 | train_loss=0.022889 | val_loss=0.016137\n",
      "Epoch 029 | train_loss=0.022670 | val_loss=0.017396\n",
      "Epoch 030 | train_loss=0.022240 | val_loss=0.017454\n",
      "Epoch 031 | train_loss=0.022119 | val_loss=0.015445\n",
      "Epoch 032 | train_loss=0.021485 | val_loss=0.017182\n",
      "Epoch 033 | train_loss=0.021356 | val_loss=0.016386\n",
      "Epoch 034 | train_loss=0.020800 | val_loss=0.015410\n",
      "Epoch 035 | train_loss=0.020365 | val_loss=0.015927\n",
      "Epoch 036 | train_loss=0.020226 | val_loss=0.016316\n",
      "Epoch 037 | train_loss=0.019938 | val_loss=0.018006\n",
      "Epoch 038 | train_loss=0.020043 | val_loss=0.016617\n",
      "Epoch 039 | train_loss=0.019643 | val_loss=0.015612\n",
      "Epoch 040 | train_loss=0.019132 | val_loss=0.015687\n",
      "Epoch 041 | train_loss=0.019096 | val_loss=0.015347\n",
      "Epoch 042 | train_loss=0.018911 | val_loss=0.016717\n",
      "Epoch 043 | train_loss=0.018853 | val_loss=0.017492\n",
      "Epoch 044 | train_loss=0.018921 | val_loss=0.015184\n",
      "Epoch 045 | train_loss=0.018394 | val_loss=0.015370\n",
      "Epoch 046 | train_loss=0.018165 | val_loss=0.017291\n",
      "Epoch 047 | train_loss=0.017766 | val_loss=0.016376\n",
      "Epoch 048 | train_loss=0.017745 | val_loss=0.016016\n",
      "Epoch 049 | train_loss=0.017718 | val_loss=0.016816\n",
      "Epoch 050 | train_loss=0.017337 | val_loss=0.015076\n",
      "Epoch 051 | train_loss=0.017059 | val_loss=0.015299\n",
      "Epoch 052 | train_loss=0.016931 | val_loss=0.015557\n",
      "Epoch 053 | train_loss=0.016677 | val_loss=0.015534\n",
      "Epoch 054 | train_loss=0.016572 | val_loss=0.015774\n",
      "Epoch 055 | train_loss=0.016627 | val_loss=0.015214\n",
      "Epoch 056 | train_loss=0.016215 | val_loss=0.016528\n",
      "Epoch 057 | train_loss=0.016361 | val_loss=0.016109\n",
      "Early stopping triggered.\n",
      "\n",
      "=== Autoencoder (PyTorch) ===\n",
      "{'model': 'Autoencoder_Torch', 'tn': 19962, 'fp': 38, 'fn': 183, 'tp': 417, 'accuracy': 0.9892718446601941, 'precision': 0.9164835164835164, 'recall': 0.695, 'f1': 0.790521327014218, 'roc_auc': 0.9561054583333333, 'pr_auc': 0.7828537619722353, 'threshold': 0.07280096191488489}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9981    0.9945     20000\n",
      "           1     0.9165    0.6950    0.7905       600\n",
      "\n",
      "    accuracy                         0.9893     20600\n",
      "   macro avg     0.9537    0.8465    0.8925     20600\n",
      "weighted avg     0.9887    0.9893    0.9886     20600\n",
      "\n",
      "\n",
      "✅ DONE. Saved outputs to: c:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\outputs\n",
      " - Per-model folders contain: confusion_matrix.csv, metrics.json, classification_report.txt\n",
      " - Summary: summary_all_models.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# PYTORCH + SKLEARN ANOMALY DETECTION PIPELINE (MIXED DTYPES)\n",
    "# Works with your dataset columns (TARGET = is_attack)\n",
    "# Handles categorical/string columns via OneHotEncoder\n",
    "# =========================================================\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 1 — IMPORTS + SETTINGS\n",
    "# =========================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_PATH = r\"E:\\NIT surathkal\\data.csv\"\n",
    "TARGET = \"is_attack\"                 # ✅ FIXED (your file has is_attack, not is_anomalous)\n",
    "INCLUDE_RISK_FEATURE = False         # keep False for now\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 2 — HELPERS (save + metrics)\n",
    "# =========================================================\n",
    "def to_py(obj):\n",
    "    if isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    if isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    return obj\n",
    "\n",
    "def save_json(path, payload):\n",
    "    def convert(x):\n",
    "        if isinstance(x, dict):\n",
    "            return {str(to_py(k)): convert(v) for k, v in x.items()}\n",
    "        if isinstance(x, list):\n",
    "            return [convert(v) for v in x]\n",
    "        return to_py(x)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(convert(payload), f, indent=2)\n",
    "\n",
    "def evaluate_from_scores(y_true, scores, threshold, model_name, out_dir):\n",
    "    \"\"\"\n",
    "    scores: higher => more anomalous\n",
    "    threshold: predict anomaly if score > threshold\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    scores = np.asarray(scores).astype(float)\n",
    "    y_pred = (scores > threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, scores)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, scores)),\n",
    "        \"threshold\": float(threshold),\n",
    "    }\n",
    "\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "    # Save confusion matrix\n",
    "    pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"true_0_normal\", \"true_1_attack\"],\n",
    "        columns=[\"pred_0_normal\", \"pred_1_attack\"]\n",
    "    ).to_csv(os.path.join(out_dir, \"confusion_matrix.csv\"), index=False)\n",
    "\n",
    "    save_json(os.path.join(out_dir, \"metrics.json\"), metrics)\n",
    "\n",
    "    with open(os.path.join(out_dir, \"classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    return metrics, cm, report\n",
    "\n",
    "def tune_threshold_by_f1(y_true, scores, percentiles=np.linspace(80, 99.9, 80)):\n",
    "    \"\"\"\n",
    "    Pick threshold that maximizes F1 on validation data.\n",
    "    scores: higher => more anomalous\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    scores = np.asarray(scores).astype(float)\n",
    "\n",
    "    best = {\"f1\": -1, \"threshold\": None, \"precision\": None, \"recall\": None, \"percentile\": None}\n",
    "    for p in percentiles:\n",
    "        thr = np.percentile(scores, p)\n",
    "        y_pred = (scores > thr).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\n",
    "                \"f1\": float(f1),\n",
    "                \"threshold\": float(thr),\n",
    "                \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "                \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "                \"percentile\": float(p),\n",
    "            })\n",
    "    return best\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 3 — LOAD DATA\n",
    "# =========================================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"TARGET='{TARGET}' not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"Loaded:\", DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Attack rate:\", df[TARGET].mean())\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 4 — FEATURES / LABELS (DROP LEAKY / ID COLS)\n",
    "# =========================================================\n",
    "drop_cols = [TARGET]\n",
    "\n",
    "# drop common index column\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    drop_cols.append(\"Unnamed: 0\")\n",
    "\n",
    "# drop leakage column (often only present when attack)\n",
    "if \"attack_type\" in df.columns:\n",
    "    drop_cols.append(\"attack_type\")\n",
    "\n",
    "# drop identifiers (recommended)\n",
    "for c in [\"user_id\", \"ip_address\", \"device_id\", \"timestamp\"]:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "\n",
    "# optional risk_score exclusion\n",
    "if not INCLUDE_RISK_FEATURE and \"risk_score\" in df.columns:\n",
    "    drop_cols.append(\"risk_score\")\n",
    "\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[TARGET].astype(int).values\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 5 — TRAIN/VAL/TEST SPLIT\n",
    "# =========================================================\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.20, random_state=SEED, stratify=y_train_full\n",
    ")\n",
    "\n",
    "# Unsupervised training: fit ONLY on NORMAL (non-attack) samples\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"Train full:\", X_train_full.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
    "print(\"Train normals only:\", X_train_normal.shape)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 6 — PREPROCESSOR (NUMERIC + CATEGORICAL SAFE)\n",
    "# =========================================================\n",
    "num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(\"\\nFeature types:\")\n",
    "print(\"Numeric:\", len(num_cols), num_cols)\n",
    "print(\"Categorical:\", len(cat_cols), cat_cols)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(with_centering=True, with_scaling=True), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train_normal_p = preprocessor.fit_transform(X_train_normal)\n",
    "X_val_p = preprocessor.transform(X_val)\n",
    "X_test_p = preprocessor.transform(X_test)\n",
    "\n",
    "# Save preprocessed splits (optional)\n",
    "pd.DataFrame(X_train_normal_p).to_csv(os.path.join(OUTPUT_DIR, \"X_train_normal_preprocessed.csv\"), index=False)\n",
    "pd.DataFrame(X_val_p).to_csv(os.path.join(OUTPUT_DIR, \"X_val_preprocessed.csv\"), index=False)\n",
    "pd.DataFrame(X_test_p).to_csv(os.path.join(OUTPUT_DIR, \"X_test_preprocessed.csv\"), index=False)\n",
    "pd.Series(y_val, name=\"y_val\").to_csv(os.path.join(OUTPUT_DIR, \"y_val.csv\"), index=False)\n",
    "pd.Series(y_test, name=\"y_test\").to_csv(os.path.join(OUTPUT_DIR, \"y_test.csv\"), index=False)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 7 — MODEL 1: ISOLATION FOREST\n",
    "# =========================================================\n",
    "iso_dir = os.path.join(OUTPUT_DIR, \"IsolationForest\")\n",
    "os.makedirs(iso_dir, exist_ok=True)\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=400,\n",
    "    max_samples=\"auto\",\n",
    "    contamination=\"auto\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "iso.fit(X_train_normal_p)\n",
    "\n",
    "val_scores_iso = -iso.decision_function(X_val_p)\n",
    "test_scores_iso = -iso.decision_function(X_test_p)\n",
    "\n",
    "best_iso = tune_threshold_by_f1(y_val, val_scores_iso)\n",
    "metrics_iso, cm_iso, rep_iso = evaluate_from_scores(y_test, test_scores_iso, best_iso[\"threshold\"], \"IsolationForest\", iso_dir)\n",
    "save_json(os.path.join(iso_dir, \"threshold_tuning.json\"), best_iso)\n",
    "\n",
    "print(\"\\n=== IsolationForest ===\")\n",
    "print(metrics_iso)\n",
    "print(rep_iso)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 8 — MODEL 2: ONE-CLASS SVM\n",
    "# =========================================================\n",
    "svm_dir = os.path.join(OUTPUT_DIR, \"OneClassSVM\")\n",
    "os.makedirs(svm_dir, exist_ok=True)\n",
    "\n",
    "ocsvm = OneClassSVM(kernel=\"rbf\", nu=0.05, gamma=\"scale\")\n",
    "ocsvm.fit(X_train_normal_p)\n",
    "\n",
    "val_scores_svm = -ocsvm.decision_function(X_val_p)\n",
    "test_scores_svm = -ocsvm.decision_function(X_test_p)\n",
    "\n",
    "best_svm = tune_threshold_by_f1(y_val, val_scores_svm)\n",
    "metrics_svm, cm_svm, rep_svm = evaluate_from_scores(y_test, test_scores_svm, best_svm[\"threshold\"], \"OneClassSVM\", svm_dir)\n",
    "save_json(os.path.join(svm_dir, \"threshold_tuning.json\"), best_svm)\n",
    "\n",
    "print(\"\\n=== OneClassSVM ===\")\n",
    "print(metrics_svm)\n",
    "print(rep_svm)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 9 — MODEL 3: LOCAL OUTLIER FACTOR (NOVELTY MODE)\n",
    "# =========================================================\n",
    "lof_dir = os.path.join(OUTPUT_DIR, \"LocalOutlierFactor\")\n",
    "os.makedirs(lof_dir, exist_ok=True)\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=35,\n",
    "    novelty=True,\n",
    "    metric=\"minkowski\"\n",
    ")\n",
    "lof.fit(X_train_normal_p)\n",
    "\n",
    "val_scores_lof = -lof.decision_function(X_val_p)\n",
    "test_scores_lof = -lof.decision_function(X_test_p)\n",
    "\n",
    "best_lof = tune_threshold_by_f1(y_val, val_scores_lof)\n",
    "metrics_lof, cm_lof, rep_lof = evaluate_from_scores(y_test, test_scores_lof, best_lof[\"threshold\"], \"LocalOutlierFactor\", lof_dir)\n",
    "save_json(os.path.join(lof_dir, \"threshold_tuning.json\"), best_lof)\n",
    "\n",
    "print(\"\\n=== LocalOutlierFactor ===\")\n",
    "print(metrics_lof)\n",
    "print(rep_lof)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 10 — MODEL 4: AUTOENCODER (PYTORCH)\n",
    "# =========================================================\n",
    "ae_dir = os.path.join(OUTPUT_DIR, \"Autoencoder_Torch\")\n",
    "os.makedirs(ae_dir, exist_ok=True)\n",
    "\n",
    "input_dim = X_train_normal_p.shape[1]\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.15),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.10),\n",
    "\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.10),\n",
    "\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def mae_per_row(recon, x):\n",
    "    return torch.mean(torch.abs(recon - x), dim=1)\n",
    "\n",
    "def train_ae_min_epoch_early_stop(\n",
    "    model, train_loader, val_tensor,\n",
    "    min_epochs=15, patience=7, max_epochs=60, lr=1e-3\n",
    "):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.L1Loss()  # MAE\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    best_val = float(\"inf\")\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for (xb,) in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            recon = model(xb)\n",
    "            loss = criterion(recon, xb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xv = val_tensor.to(DEVICE)\n",
    "            rv = model(xv)\n",
    "            vloss = criterion(rv, xv).item()\n",
    "\n",
    "        tloss = float(np.mean(losses))\n",
    "        history[\"train_loss\"].append(tloss)\n",
    "        history[\"val_loss\"].append(vloss)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tloss:.6f} | val_loss={vloss:.6f}\")\n",
    "\n",
    "        if vloss < best_val - 1e-8:\n",
    "            best_val = vloss\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), os.path.join(ae_dir, \"best_ae.pt\"))\n",
    "        else:\n",
    "            if epoch >= min_epochs:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    best_path = os.path.join(ae_dir, \"best_ae.pt\")\n",
    "    if os.path.exists(best_path):\n",
    "        model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "\n",
    "    return history\n",
    "\n",
    "# Torch tensors\n",
    "X_train_t = torch.tensor(X_train_normal_p, dtype=torch.float32)\n",
    "X_val_t = torch.tensor(X_val_p, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test_p, dtype=torch.float32)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t), batch_size=1024, shuffle=True, drop_last=False)\n",
    "\n",
    "ae = AutoEncoder(input_dim).to(DEVICE)\n",
    "hist = train_ae_min_epoch_early_stop(ae, train_loader, X_val_t, min_epochs=15, patience=7, max_epochs=60, lr=1e-3)\n",
    "\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    val_recon = mae_per_row(ae(X_val_t.to(DEVICE)), X_val_t.to(DEVICE)).cpu().numpy()\n",
    "    test_recon = mae_per_row(ae(X_test_t.to(DEVICE)), X_test_t.to(DEVICE)).cpu().numpy()\n",
    "\n",
    "best_ae = tune_threshold_by_f1(y_val, val_recon)\n",
    "metrics_ae, cm_ae, rep_ae = evaluate_from_scores(y_test, test_recon, best_ae[\"threshold\"], \"Autoencoder_Torch\", ae_dir)\n",
    "save_json(os.path.join(ae_dir, \"threshold_tuning.json\"), best_ae)\n",
    "\n",
    "pd.DataFrame(hist).to_csv(os.path.join(ae_dir, \"train_history.csv\"), index=False)\n",
    "\n",
    "print(\"\\n=== Autoencoder (PyTorch) ===\")\n",
    "print(metrics_ae)\n",
    "print(rep_ae)\n",
    "\n",
    "# =========================================================\n",
    "# SECTION 11 — FINAL SUMMARY\n",
    "# =========================================================\n",
    "summary = pd.DataFrame([metrics_iso, metrics_svm, metrics_lof, metrics_ae])\n",
    "summary.to_csv(os.path.join(OUTPUT_DIR, \"summary_all_models.csv\"), index=False)\n",
    "\n",
    "print(\"\\n✅ DONE. Saved outputs to:\", os.path.abspath(OUTPUT_DIR))\n",
    "print(\" - Per-model folders contain: confusion_matrix.csv, metrics.json, classification_report.txt\")\n",
    "print(\" - Summary: summary_all_models.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\data.csv\n",
      "Shape: (103000, 18)\n",
      "Attack rate: 0.02912621359223301\n",
      "\n",
      "Split sizes:\n",
      "Train full: (82400, 11) Test: (20600, 11)\n",
      "Train: (65920, 11) Val: (16480, 11)\n",
      "Train normals only: (64000, 11)\n",
      "\n",
      "Numeric cols: 8\n",
      "Categorical cols: 3\n",
      "\n",
      "Transformed shapes:\n",
      "X_train_normal_p: (64000, 16)\n",
      "X_train_full_p:   (82400, 16)\n",
      "X_val_p:          (16480, 16)\n",
      "X_test_p:         (20600, 16)\n",
      "\n",
      "Meta-feature shapes:\n",
      "train: (82400, 3, 3) val: (16480, 3, 3) test: (20600, 3, 3)\n",
      "Epoch 01 | loss=0.0818 | val_f1=0.7689 | val_pr_auc=0.8702\n",
      "Epoch 02 | loss=0.0384 | val_f1=0.7799 | val_pr_auc=0.8740\n",
      "Epoch 03 | loss=0.0318 | val_f1=0.8069 | val_pr_auc=0.8848\n",
      "Epoch 04 | loss=0.0281 | val_f1=0.8089 | val_pr_auc=0.8931\n",
      "Epoch 05 | loss=0.0269 | val_f1=0.8089 | val_pr_auc=0.9025\n",
      "Epoch 06 | loss=0.0268 | val_f1=0.8049 | val_pr_auc=0.8921\n",
      "Epoch 07 | loss=0.0258 | val_f1=0.8094 | val_pr_auc=0.9116\n",
      "Epoch 08 | loss=0.0262 | val_f1=0.8089 | val_pr_auc=0.9043\n",
      "Epoch 09 | loss=0.0257 | val_f1=0.8089 | val_pr_auc=0.9051\n",
      "Epoch 10 | loss=0.0256 | val_f1=0.8089 | val_pr_auc=0.9122\n",
      "Epoch 11 | loss=0.0250 | val_f1=0.8089 | val_pr_auc=0.9140\n",
      "Epoch 12 | loss=0.0251 | val_f1=0.8089 | val_pr_auc=0.8852\n",
      "Epoch 13 | loss=0.0250 | val_f1=0.8089 | val_pr_auc=0.9141\n",
      "Epoch 14 | loss=0.0244 | val_f1=0.8089 | val_pr_auc=0.9132\n",
      "Epoch 15 | loss=0.0245 | val_f1=0.8089 | val_pr_auc=0.9160\n",
      "Early stopping.\n",
      "\n",
      "Best VAL threshold by F1: {'f1': 0.8144690781796966, 'threshold': 0.19, 'precision': 0.9257294429708223, 'recall': 0.7270833333333333}\n",
      "\n",
      "=== Attention-Only Fusion TEST @ thr=0.19 ===\n",
      "Accuracy : 0.9906796116504855\n",
      "Precision: 0.9112903225806451\n",
      "Recall   : 0.7533333333333333\n",
      "F1       : 0.8248175182481752\n",
      "ROC-AUC  : 0.9960966666666666\n",
      "PR-AUC   : 0.9169740766418897\n",
      "\n",
      "Confusion Matrix:\n",
      " [[19956    44]\n",
      " [  148   452]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9926    0.9978    0.9952     20000\n",
      "           1     0.9113    0.7533    0.8248       600\n",
      "\n",
      "    accuracy                         0.9907     20600\n",
      "   macro avg     0.9520    0.8756    0.9100     20600\n",
      "weighted avg     0.9903    0.9907    0.9902     20600\n",
      "\n",
      "\n",
      "✅ DONE. Outputs saved to: c:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\outputs_attention_only\\Fusion_Attention_Only\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FULL PIPELINE (ATTENTION ONLY — NO GATED FUSION):\n",
    "# 1) Load data (TARGET = is_attack)\n",
    "# 2) Split train/val/test (stratified)\n",
    "# 3) Fit preprocessor on TRAIN NORMAL ONLY (robust scale + onehot)\n",
    "# 4) Train 3 unsupervised models on TRAIN NORMAL ONLY:\n",
    "#       IsolationForest, OneClassSVM, LOF(novelty)\n",
    "# 5) Extract meta-features from the 3 models for train_full/val/test\n",
    "# 6) Train ATTENTION-ONLY fusion (PyTorch) supervised using y labels\n",
    "# 7) Evaluate on test + print metrics + save outputs\n",
    "# =========================================================\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -----------------------------\n",
    "# SETTINGS\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\data.csv\"\n",
    "TARGET = \"is_attack\"\n",
    "OUTPUT_DIR = \"./outputs_attention_only\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def evaluate_probs(y_true, probs, threshold=0.5):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    probs = np.asarray(probs).astype(float)\n",
    "    y_pred = (probs >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, probs)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, probs)),\n",
    "        \"threshold\": float(threshold),\n",
    "    }\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    return metrics, cm, report\n",
    "\n",
    "def save_text(path, s):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(s)\n",
    "\n",
    "def save_json(path, payload):\n",
    "    def to_py(obj):\n",
    "        if isinstance(obj, (np.integer,)): return int(obj)\n",
    "        if isinstance(obj, (np.floating,)): return float(obj)\n",
    "        if isinstance(obj, (np.ndarray,)): return obj.tolist()\n",
    "        return obj\n",
    "    def convert(x):\n",
    "        if isinstance(x, dict):\n",
    "            return {str(to_py(k)): convert(v) for k, v in x.items()}\n",
    "        if isinstance(x, list):\n",
    "            return [convert(v) for v in x]\n",
    "        return to_py(x)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(convert(payload), f, indent=2)\n",
    "\n",
    "def tune_threshold_by_f1_probs(y_true, probs, thresholds=np.linspace(0.01, 0.99, 99)):\n",
    "    best = {\"f1\": -1, \"threshold\": 0.5, \"precision\": 0, \"recall\": 0}\n",
    "    for t in thresholds:\n",
    "        pred = (probs >= t).astype(int)\n",
    "        f1 = f1_score(y_true, pred, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\n",
    "                \"f1\": float(f1),\n",
    "                \"threshold\": float(t),\n",
    "                \"precision\": float(precision_score(y_true, pred, zero_division=0)),\n",
    "                \"recall\": float(recall_score(y_true, pred, zero_division=0)),\n",
    "            }\n",
    "    return best\n",
    "\n",
    "# =========================================================\n",
    "# 1) LOAD DATA\n",
    "# =========================================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"TARGET='{TARGET}' not found. Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"Loaded:\", DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Attack rate:\", df[TARGET].mean())\n",
    "\n",
    "# =========================================================\n",
    "# 2) FEATURES / LABELS (DROP LEAKY + IDS)\n",
    "# =========================================================\n",
    "drop_cols = [TARGET]\n",
    "\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    drop_cols.append(\"Unnamed: 0\")\n",
    "\n",
    "if \"attack_type\" in df.columns:\n",
    "    drop_cols.append(\"attack_type\")\n",
    "\n",
    "for c in [\"user_id\", \"ip_address\", \"device_id\", \"timestamp\"]:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[TARGET].astype(int).values\n",
    "\n",
    "# =========================================================\n",
    "# 3) SPLIT train/val/test\n",
    "# =========================================================\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.20, random_state=SEED, stratify=y_train_full\n",
    ")\n",
    "\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"Train full:\", X_train_full.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
    "print(\"Train normals only:\", X_train_normal.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 4) PREPROCESSOR (fit on normal training only)\n",
    "# =========================================================\n",
    "num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(\"\\nNumeric cols:\", len(num_cols))\n",
    "print(\"Categorical cols:\", len(cat_cols))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(with_centering=True, with_scaling=True), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train_normal_p = preprocessor.fit_transform(X_train_normal)\n",
    "X_train_full_p   = preprocessor.transform(X_train_full)\n",
    "X_val_p          = preprocessor.transform(X_val)\n",
    "X_test_p         = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nTransformed shapes:\")\n",
    "print(\"X_train_normal_p:\", X_train_normal_p.shape)\n",
    "print(\"X_train_full_p:  \", X_train_full_p.shape)\n",
    "print(\"X_val_p:         \", X_val_p.shape)\n",
    "print(\"X_test_p:        \", X_test_p.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 5) TRAIN 3 UNSUPERVISED MODELS (on normals)\n",
    "# =========================================================\n",
    "iso = IsolationForest(\n",
    "    n_estimators=400,\n",
    "    max_samples=\"auto\",\n",
    "    contamination=\"auto\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "ocsvm = OneClassSVM(kernel=\"rbf\", nu=0.05, gamma=\"scale\")\n",
    "lof = LocalOutlierFactor(n_neighbors=35, novelty=True, metric=\"minkowski\")\n",
    "\n",
    "iso.fit(X_train_normal_p)\n",
    "ocsvm.fit(X_train_normal_p)\n",
    "lof.fit(X_train_normal_p)\n",
    "\n",
    "def anomaly_scores(model, Xp):\n",
    "    return -model.decision_function(Xp)  # higher => more anomalous\n",
    "\n",
    "# =========================================================\n",
    "# 6) META-FEATURES FROM 3 MODELS  => [N, 3, 3]\n",
    "# =========================================================\n",
    "def build_meta_features(Xp, score_stats=None):\n",
    "    s_iso = anomaly_scores(iso, Xp)\n",
    "    s_svm = anomaly_scores(ocsvm, Xp)\n",
    "    s_lof = anomaly_scores(lof, Xp)\n",
    "\n",
    "    S = np.vstack([s_iso, s_svm, s_lof]).T  # [N, 3]\n",
    "\n",
    "    if score_stats is None:\n",
    "        mu = S.mean(axis=0, keepdims=True)\n",
    "        sd = S.std(axis=0, keepdims=True) + 1e-8\n",
    "        score_stats = (mu, sd)\n",
    "    else:\n",
    "        mu, sd = score_stats\n",
    "\n",
    "    Z = (S - mu) / sd  # [N, 3]\n",
    "\n",
    "    R = np.zeros_like(S)\n",
    "    for j in range(S.shape[1]):\n",
    "        order = np.argsort(S[:, j])\n",
    "        ranks = np.empty_like(order, dtype=float)\n",
    "        ranks[order] = np.linspace(0, 1, len(order))\n",
    "        R[:, j] = ranks\n",
    "\n",
    "    feats = np.stack([S, Z, R], axis=-1)  # [N, 3, 3]\n",
    "    return feats.astype(np.float32), score_stats\n",
    "\n",
    "X_train_full_feats, stats = build_meta_features(X_train_full_p, score_stats=None)\n",
    "X_val_feats, _ = build_meta_features(X_val_p, score_stats=stats)\n",
    "X_test_feats, _ = build_meta_features(X_test_p, score_stats=stats)\n",
    "\n",
    "print(\"\\nMeta-feature shapes:\")\n",
    "print(\"train:\", X_train_full_feats.shape, \"val:\", X_val_feats.shape, \"test:\", X_test_feats.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 7) ATTENTION-ONLY FUSION MODEL (NO GATE)\n",
    "# =========================================================\n",
    "class AttentionOnlyFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  [B, M, D]  M=3 models, D=3 meta-features\n",
    "    Output: logit [B]\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in=3, d_model=32, n_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_model)\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=n_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.head = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x)           # [B, M, d_model]\n",
    "        a, _ = self.attn(h, h, h)  # [B, M, d_model]\n",
    "        fused = a.mean(dim=1)      # [B, d_model]\n",
    "        fused = self.pool(fused)\n",
    "        logit = self.head(fused).squeeze(1)\n",
    "        return logit\n",
    "\n",
    "def make_loader(X_feats, y, batch_size=512, shuffle=True):\n",
    "    Xt = torch.tensor(X_feats, dtype=torch.float32)\n",
    "    yt = torch.tensor(y.astype(np.float32), dtype=torch.float32)\n",
    "    return DataLoader(TensorDataset(Xt, yt), batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "train_loader = make_loader(X_train_full_feats, y_train_full, batch_size=512, shuffle=True)\n",
    "val_loader = make_loader(X_val_feats, y_val, batch_size=1024, shuffle=False)\n",
    "test_loader = make_loader(X_test_feats, y_test, batch_size=1024, shuffle=False)\n",
    "\n",
    "model = AttentionOnlyFusion(d_in=3, d_model=32, n_heads=4, dropout=0.1).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def predict_probs(loader):\n",
    "    model.eval()\n",
    "    all_logits, all_y = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_y.append(yb.numpy())\n",
    "    logits = np.concatenate(all_logits)\n",
    "    y = np.concatenate(all_y).astype(int)\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    return y, probs\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_state = None\n",
    "patience = 8\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    yv, pv = predict_probs(val_loader)\n",
    "    val_metrics, _, _ = evaluate_probs(yv, pv, threshold=0.5)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | loss={np.mean(losses):.4f} | val_f1={val_metrics['f1']:.4f} | val_pr_auc={val_metrics['pr_auc']:.4f}\")\n",
    "\n",
    "    if val_metrics[\"f1\"] > best_f1 + 1e-6:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# =========================================================\n",
    "# 8) THRESHOLD TUNING ON VAL + TEST METRICS\n",
    "# =========================================================\n",
    "yv, pv = predict_probs(val_loader)\n",
    "best_thr = tune_threshold_by_f1_probs(yv, pv)\n",
    "print(\"\\nBest VAL threshold by F1:\", best_thr)\n",
    "\n",
    "yt, pt = predict_probs(test_loader)\n",
    "test_metrics, test_cm, test_report = evaluate_probs(yt, pt, threshold=best_thr[\"threshold\"])\n",
    "\n",
    "print(f\"\\n=== Attention-Only Fusion TEST @ thr={best_thr['threshold']:.2f} ===\")\n",
    "print(\"Accuracy :\", test_metrics[\"accuracy\"])\n",
    "print(\"Precision:\", test_metrics[\"precision\"])\n",
    "print(\"Recall   :\", test_metrics[\"recall\"])\n",
    "print(\"F1       :\", test_metrics[\"f1\"])\n",
    "print(\"ROC-AUC  :\", test_metrics[\"roc_auc\"])\n",
    "print(\"PR-AUC   :\", test_metrics[\"pr_auc\"])\n",
    "print(\"\\nConfusion Matrix:\\n\", test_cm)\n",
    "print(\"\\nClassification Report:\\n\", test_report)\n",
    "\n",
    "# =========================================================\n",
    "# 9) SAVE OUTPUTS\n",
    "# =========================================================\n",
    "fusion_dir = os.path.join(OUTPUT_DIR, \"Fusion_Attention_Only\")\n",
    "os.makedirs(fusion_dir, exist_ok=True)\n",
    "\n",
    "pd.DataFrame(\n",
    "    test_cm,\n",
    "    index=[\"true_0_normal\", \"true_1_attack\"],\n",
    "    columns=[\"pred_0_normal\", \"pred_1_attack\"]\n",
    ").to_csv(os.path.join(fusion_dir, \"confusion_matrix.csv\"))\n",
    "\n",
    "save_json(os.path.join(fusion_dir, \"metrics.json\"), test_metrics)\n",
    "save_text(os.path.join(fusion_dir, \"classification_report.txt\"), test_report)\n",
    "\n",
    "pd.DataFrame({\"y_true\": yt, \"p_attack\": pt}).to_csv(os.path.join(fusion_dir, \"test_predictions.csv\"), index=False)\n",
    "\n",
    "print(\"\\n✅ DONE. Outputs saved to:\", os.path.abspath(fusion_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c916dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ VALIDATION METRICS ================\n",
      "Threshold : 0.19\n",
      "Accuracy  : 0.9904\n",
      "Precision : 0.9257\n",
      "Recall    : 0.7271\n",
      "F1-score  : 0.8145\n",
      "ROC-AUC   : 0.9957\n",
      "PR-AUC    : 0.9116\n",
      "Confusion Matrix:\n",
      " [[15972    28]\n",
      " [  131   349]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9919    0.9982    0.9950     16000\n",
      "           1     0.9257    0.7271    0.8145       480\n",
      "\n",
      "    accuracy                         0.9904     16480\n",
      "   macro avg     0.9588    0.8627    0.9048     16480\n",
      "weighted avg     0.9899    0.9904    0.9898     16480\n",
      "\n",
      "\n",
      "=================== TEST METRICS ===================\n",
      "Threshold : 0.19\n",
      "Accuracy  : 0.9907\n",
      "Precision : 0.9113\n",
      "Recall    : 0.7533\n",
      "F1-score  : 0.8248\n",
      "ROC-AUC   : 0.9961\n",
      "PR-AUC    : 0.9170\n",
      "Confusion Matrix:\n",
      " [[19956    44]\n",
      " [  148   452]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9926    0.9978    0.9952     20000\n",
      "           1     0.9113    0.7533    0.8248       600\n",
      "\n",
      "    accuracy                         0.9907     20600\n",
      "   macro avg     0.9520    0.8756    0.9100     20600\n",
      "weighted avg     0.9903    0.9907    0.9902     20600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# PRINT METRICS (VAL + TEST)\n",
    "# ============================\n",
    "\n",
    "# 1) Validation metrics @ tuned threshold\n",
    "val_metrics, val_cm, val_report = evaluate_probs(yv, pv, threshold=best_thr[\"threshold\"])\n",
    "\n",
    "print(\"\\n================ VALIDATION METRICS ================\")\n",
    "print(f\"Threshold : {best_thr['threshold']:.2f}\")\n",
    "print(f\"Accuracy  : {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision : {val_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {val_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {val_metrics['f1']:.4f}\")\n",
    "print(f\"ROC-AUC   : {val_metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC    : {val_metrics['pr_auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", val_cm)\n",
    "print(\"\\nClassification Report:\\n\", val_report)\n",
    "\n",
    "# 2) Test metrics @ tuned threshold\n",
    "test_metrics, test_cm, test_report = evaluate_probs(yt, pt, threshold=best_thr[\"threshold\"])\n",
    "\n",
    "print(\"\\n=================== TEST METRICS ===================\")\n",
    "print(f\"Threshold : {best_thr['threshold']:.2f}\")\n",
    "print(f\"Accuracy  : {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision : {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall    : {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score  : {test_metrics['f1']:.4f}\")\n",
    "print(f\"ROC-AUC   : {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"PR-AUC    : {test_metrics['pr_auc']:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_cm)\n",
    "print(\"\\nClassification Report:\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4fb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-feature tensor shape: (82400, 3, 3)\n",
      "Models (tokens): 3\n",
      "Features per model: 3\n",
      "Total features per sample (flattened): 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Meta-feature tensor shape:\", X_train_full_feats.shape)  # (N, 3, 3)\n",
    "print(\"Models (tokens):\", X_train_full_feats.shape[1])         # 3\n",
    "print(\"Features per model:\", X_train_full_feats.shape[2])      # 3\n",
    "print(\"Total features per sample (flattened):\", X_train_full_feats.shape[1] * X_train_full_feats.shape[2])  # 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a605d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened shape: (82400, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train_flat = X_train_full_feats.reshape(X_train_full_feats.shape[0], -1)\n",
    "print(\"Flattened shape:\", X_train_flat.shape)  # (N, 9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35155b",
   "metadata": {},
   "source": [
    "# Features extracted with variational AE with transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf65caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: C:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\data.csv\n",
      "Shape: (103000, 18)\n",
      "Attack rate: 0.02912621359223301\n",
      "\n",
      "Split sizes:\n",
      "Train full: (82400, 11) Test: (20600, 11)\n",
      "Train: (65920, 11) Val: (16480, 11)\n",
      "Train normals only: (64000, 11)\n",
      "\n",
      "Preprocessed dims: 16\n",
      "\n",
      "Training Transformer-VAE on NORMAL samples...\n",
      "Epoch 01 | train=0.166227 | val=0.135743 | recon=0.108393 | kl=0.054700\n",
      "Epoch 02 | train=0.130481 | val=0.129589 | recon=0.095021 | kl=0.069137\n",
      "Epoch 03 | train=0.124651 | val=0.125028 | recon=0.087346 | kl=0.075363\n",
      "Epoch 04 | train=0.118826 | val=0.120647 | recon=0.077162 | kl=0.086971\n",
      "Epoch 05 | train=0.115769 | val=0.117973 | recon=0.072682 | kl=0.090583\n",
      "Epoch 06 | train=0.113622 | val=0.116482 | recon=0.069781 | kl=0.093403\n",
      "Epoch 07 | train=0.112233 | val=0.115705 | recon=0.068639 | kl=0.094132\n",
      "Epoch 08 | train=0.111249 | val=0.114914 | recon=0.068729 | kl=0.092370\n",
      "Epoch 09 | train=0.110775 | val=0.114052 | recon=0.065036 | kl=0.098032\n",
      "Epoch 10 | train=0.110427 | val=0.114519 | recon=0.065007 | kl=0.099024\n",
      "Epoch 11 | train=0.109919 | val=0.114207 | recon=0.065976 | kl=0.096460\n",
      "Epoch 12 | train=0.109416 | val=0.114173 | recon=0.063407 | kl=0.101531\n",
      "Epoch 13 | train=0.109557 | val=0.113231 | recon=0.063574 | kl=0.099313\n",
      "Epoch 14 | train=0.109226 | val=0.113647 | recon=0.064507 | kl=0.098280\n",
      "Epoch 15 | train=0.108947 | val=0.113542 | recon=0.064488 | kl=0.098109\n",
      "Epoch 16 | train=0.108999 | val=0.113596 | recon=0.064589 | kl=0.098013\n",
      "Epoch 17 | train=0.108710 | val=0.113688 | recon=0.064582 | kl=0.098212\n",
      "Epoch 18 | train=0.108750 | val=0.113478 | recon=0.065043 | kl=0.096870\n",
      "Epoch 19 | train=0.108570 | val=0.113380 | recon=0.063536 | kl=0.099689\n",
      "Epoch 20 | train=0.108708 | val=0.113434 | recon=0.063990 | kl=0.098887\n",
      "Epoch 21 | train=0.108351 | val=0.113274 | recon=0.062283 | kl=0.101982\n",
      "Early stopping VAE.\n",
      "\n",
      "Extracted feature shape (latent mu): (82400, 32)\n",
      "=> Number of extracted features per sample = 32\n",
      "\n",
      "================ VAE Reconstruction Error ================\n",
      "Accuracy : 0.9899\n",
      "Precision: 0.9414\n",
      "Recall   : 0.6967\n",
      "F1       : 0.8008\n",
      "ROC-AUC  : 0.9852\n",
      "PR-AUC   : 0.8347\n",
      "Threshold: 0.319551\n",
      "Confusion Matrix:\n",
      " [[19974    26]\n",
      " [  182   418]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9910    0.9987    0.9948     20000\n",
      "           1     0.9414    0.6967    0.8008       600\n",
      "\n",
      "    accuracy                         0.9899     20600\n",
      "   macro avg     0.9662    0.8477    0.8978     20600\n",
      "weighted avg     0.9895    0.9899    0.9892     20600\n",
      "\n",
      "\n",
      "================ IsolationForest on VAE Features ================\n",
      "Accuracy : 0.9884\n",
      "Precision: 0.9151\n",
      "Recall   : 0.6650\n",
      "F1       : 0.7703\n",
      "ROC-AUC  : 0.9129\n",
      "PR-AUC   : 0.7277\n",
      "Threshold: 0.050769\n",
      "Confusion Matrix:\n",
      " [[19963    37]\n",
      " [  201   399]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9900    0.9981    0.9941     20000\n",
      "           1     0.9151    0.6650    0.7703       600\n",
      "\n",
      "    accuracy                         0.9884     20600\n",
      "   macro avg     0.9526    0.8316    0.8822     20600\n",
      "weighted avg     0.9879    0.9884    0.9876     20600\n",
      "\n",
      "\n",
      "================ OneClassSVM on VAE Features ================\n",
      "Accuracy : 0.8233\n",
      "Precision: 0.0100\n",
      "Recall   : 0.0517\n",
      "F1       : 0.0167\n",
      "ROC-AUC  : 0.3479\n",
      "PR-AUC   : 0.0205\n",
      "Threshold: -19.045635\n",
      "Confusion Matrix:\n",
      " [[16929  3071]\n",
      " [  569    31]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9675    0.8465    0.9029     20000\n",
      "           1     0.0100    0.0517    0.0167       600\n",
      "\n",
      "    accuracy                         0.8233     20600\n",
      "   macro avg     0.4887    0.4491    0.4598     20600\n",
      "weighted avg     0.9396    0.8233    0.8771     20600\n",
      "\n",
      "\n",
      "================ LOF on VAE Features ================\n",
      "Accuracy : 0.9906\n",
      "Precision: 0.9657\n",
      "Recall   : 0.7033\n",
      "F1       : 0.8139\n",
      "ROC-AUC  : 0.8983\n",
      "PR-AUC   : 0.7490\n",
      "Threshold: -0.203004\n",
      "Confusion Matrix:\n",
      " [[19985    15]\n",
      " [  178   422]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9912    0.9992    0.9952     20000\n",
      "           1     0.9657    0.7033    0.8139       600\n",
      "\n",
      "    accuracy                         0.9906     20600\n",
      "   macro avg     0.9784    0.8513    0.9045     20600\n",
      "weighted avg     0.9904    0.9906    0.9899     20600\n",
      "\n",
      "\n",
      "✅ Saved summary: c:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\outputs_vae_transformer\\summary_metrics.csv\n",
      "                    model  accuracy  precision    recall        f1   roc_auc  \\\n",
      "0          VAE_ReconError  0.989903   0.941441  0.696667  0.800766  0.985167   \n",
      "1  IsolationForest_on_VAE  0.988447   0.915138  0.665000  0.770270  0.912891   \n",
      "2      OneClassSVM_on_VAE  0.823301   0.009994  0.051667  0.016748  0.347945   \n",
      "3              LOF_on_VAE  0.990631   0.965675  0.703333  0.813886  0.898258   \n",
      "\n",
      "     pr_auc  threshold  \n",
      "0  0.834700   0.319551  \n",
      "1  0.727716   0.050769  \n",
      "2  0.020486 -19.045635  \n",
      "3  0.749027  -0.203004  \n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TRANSFORMER VAE FEATURE EXTRACTION + 3 MODELS + METRICS\n",
    "# =========================================================\n",
    "\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -----------------------------\n",
    "# SETTINGS\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\varsh\\OneDrive\\Desktop\\NITK\\data.csv\"\n",
    "TARGET = \"is_attack\"\n",
    "OUTPUT_DIR = \"./outputs_vae_transformer\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Drop leaky / ID columns (recommended)\n",
    "DROP_IDS = [\"Unnamed: 0\", \"user_id\", \"ip_address\", \"device_id\", \"timestamp\"]\n",
    "DROP_LEAKY = [\"attack_type\"]  # often leakage\n",
    "\n",
    "# -----------------------------\n",
    "# METRICS HELPERS\n",
    "# -----------------------------\n",
    "def tune_threshold_by_f1(y_true, scores, percentiles=np.linspace(80, 99.9, 80)):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    scores = np.asarray(scores).astype(float)\n",
    "\n",
    "    best = {\"f1\": -1, \"threshold\": None, \"precision\": None, \"recall\": None, \"percentile\": None}\n",
    "    for p in percentiles:\n",
    "        thr = np.percentile(scores, p)\n",
    "        y_pred = (scores > thr).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best = {\n",
    "                \"f1\": float(f1),\n",
    "                \"threshold\": float(thr),\n",
    "                \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "                \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "                \"percentile\": float(p),\n",
    "            }\n",
    "    return best\n",
    "\n",
    "def evaluate_from_scores(y_true, scores, threshold, model_name):\n",
    "    \"\"\"\n",
    "    scores: higher => more anomalous\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    scores = np.asarray(scores).astype(float)\n",
    "    y_pred = (scores > threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, scores)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, scores)),\n",
    "        \"threshold\": float(threshold),\n",
    "    }\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    return metrics, cm, report\n",
    "\n",
    "def print_metrics_block(title, metrics, cm, report):\n",
    "    print(f\"\\n================ {title} ================\")\n",
    "    print(f\"Accuracy : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall   : {metrics['recall']:.4f}\")\n",
    "    print(f\"F1       : {metrics['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC  : {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"PR-AUC   : {metrics['pr_auc']:.4f}\")\n",
    "    print(f\"Threshold: {metrics['threshold']:.6f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# =========================================================\n",
    "# 1) LOAD DATA\n",
    "# =========================================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise ValueError(f\"TARGET='{TARGET}' not found. Available: {df.columns.tolist()}\")\n",
    "\n",
    "print(\"Loaded:\", DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Attack rate:\", df[TARGET].mean())\n",
    "\n",
    "# =========================================================\n",
    "# 2) BUILD X,y (drop ids/leaky)\n",
    "# =========================================================\n",
    "drop_cols = [TARGET]\n",
    "for c in DROP_IDS + DROP_LEAKY:\n",
    "    if c in df.columns:\n",
    "        drop_cols.append(c)\n",
    "\n",
    "X = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "y = df[TARGET].astype(int).values\n",
    "\n",
    "# =========================================================\n",
    "# 3) SPLIT train/val/test\n",
    "# =========================================================\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=SEED, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.20, random_state=SEED, stratify=y_train_full\n",
    ")\n",
    "\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"Train full:\", X_train_full.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n",
    "print(\"Train normals only:\", X_train_normal.shape)\n",
    "\n",
    "# =========================================================\n",
    "# 4) PREPROCESS (fit on normal only)\n",
    "# =========================================================\n",
    "num_cols = [c for c in X.columns if is_numeric_dtype(X[c])]\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", RobustScaler(with_centering=True, with_scaling=True), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train_normal_p = preprocessor.fit_transform(X_train_normal)\n",
    "X_train_full_p   = preprocessor.transform(X_train_full)\n",
    "X_val_p          = preprocessor.transform(X_val)\n",
    "X_test_p         = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nPreprocessed dims:\", X_train_full_p.shape[1])\n",
    "\n",
    "# =========================================================\n",
    "# 5) TRANSFORMER VAE FOR TABULAR (PATCH TRANSFORMER)\n",
    "# =========================================================\n",
    "class PatchTransformerVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Takes a dense feature vector [B, F] (after preprocessing),\n",
    "    splits into patches, projects patches to d_model,\n",
    "    TransformerEncoder -> pooled -> (mu, logvar) latent.\n",
    "    Decoder reconstructs original [B, F].\n",
    "\n",
    "    Feature extraction = mu (latent embedding).\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_dim, patch_size=32, d_model=128, n_heads=4, n_layers=2,\n",
    "                 latent_dim=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # compute patches + padding\n",
    "        self.n_patches = int(np.ceil(feat_dim / patch_size))\n",
    "        self.pad_dim = self.n_patches * patch_size\n",
    "        self.pad_needed = self.pad_dim - feat_dim\n",
    "\n",
    "        self.patch_proj = nn.Linear(patch_size, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.encoder_tf = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.mu = nn.Linear(d_model, latent_dim)\n",
    "        self.logvar = nn.Linear(d_model, latent_dim)\n",
    "\n",
    "        # Decoder: latent -> hidden -> reconstruct full padded vector -> trim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, d_model*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model*2, self.pad_dim)\n",
    "        )\n",
    "\n",
    "    def _pad(self, x):\n",
    "        if self.pad_needed <= 0:\n",
    "            return x\n",
    "        pad = torch.zeros((x.size(0), self.pad_needed), device=x.device, dtype=x.dtype)\n",
    "        return torch.cat([x, pad], dim=1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: [B, F]\n",
    "        x = self._pad(x)  # [B, pad_dim]\n",
    "        x = x.view(x.size(0), self.n_patches, self.patch_size)  # [B, P, patch]\n",
    "        h = self.patch_proj(x)  # [B, P, d_model]\n",
    "        h = self.encoder_tf(h)  # [B, P, d_model]\n",
    "        h = h.mean(dim=1)       # pooled [B, d_model]\n",
    "        h = self.pool(h)\n",
    "        mu = self.mu(h)\n",
    "        logvar = self.logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_pad = self.decoder(z)               # [B, pad_dim]\n",
    "        recon = recon_pad[:, :self.feat_dim]      # [B, F]\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def vae_loss(recon, x, mu, logvar, beta=1.0):\n",
    "    # recon loss (MSE) + beta * KL\n",
    "    recon_loss = torch.mean((recon - x) ** 2)\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl, recon_loss.detach(), kl.detach()\n",
    "\n",
    "# ---- Train VAE on normal-only data\n",
    "feat_dim = X_train_normal_p.shape[1]\n",
    "vae = PatchTransformerVAE(\n",
    "    feat_dim=feat_dim,\n",
    "    patch_size=32,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    latent_dim=32,\n",
    "    dropout=0.1\n",
    ").to(DEVICE)\n",
    "\n",
    "opt = torch.optim.AdamW(vae.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "Xtr = torch.tensor(X_train_normal_p, dtype=torch.float32)\n",
    "Xv  = torch.tensor(X_val_p, dtype=torch.float32)  # only for monitoring recon, not labels\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr), batch_size=1024, shuffle=True, drop_last=False)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "patience, wait = 8, 0\n",
    "EPOCHS = 50\n",
    "BETA = 0.5  # you can try 0.1..1.0\n",
    "\n",
    "print(\"\\nTraining Transformer-VAE on NORMAL samples...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    vae.train()\n",
    "    losses = []\n",
    "    for (xb,) in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        recon, mu, logvar = vae(xb)\n",
    "        loss, rloss, kl = vae_loss(recon, xb, mu, logvar, beta=BETA)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # quick val recon monitoring\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        xv = Xv.to(DEVICE)\n",
    "        recon_v, mu_v, logvar_v = vae(xv)\n",
    "        val_loss, val_rloss, val_kl = vae_loss(recon_v, xv, mu_v, logvar_v, beta=BETA)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train={np.mean(losses):.6f} | val={val_loss.item():.6f} | recon={val_rloss.item():.6f} | kl={val_kl.item():.6f}\")\n",
    "\n",
    "    if val_loss.item() < best_val - 1e-7:\n",
    "        best_val = val_loss.item()\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in vae.state_dict().items()}\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping VAE.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    vae.load_state_dict(best_state)\n",
    "\n",
    "# =========================================================\n",
    "# 6) FEATURE EXTRACTION FROM VAE (use mu)\n",
    "# =========================================================\n",
    "def extract_mu(Xp):\n",
    "    vae.eval()\n",
    "    Xt = torch.tensor(Xp, dtype=torch.float32, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        mu, logvar = vae.encode(Xt)\n",
    "    return mu.cpu().numpy()\n",
    "\n",
    "Z_train_full = extract_mu(X_train_full_p)\n",
    "Z_train_norm = extract_mu(X_train_normal_p)\n",
    "Z_val = extract_mu(X_val_p)\n",
    "Z_test = extract_mu(X_test_p)\n",
    "\n",
    "print(\"\\nExtracted feature shape (latent mu):\", Z_train_full.shape)\n",
    "print(\"=> Number of extracted features per sample =\", Z_train_full.shape[1])\n",
    "\n",
    "# Optional: VAE reconstruction error as anomaly score too\n",
    "def recon_error(Xp):\n",
    "    vae.eval()\n",
    "    Xt = torch.tensor(Xp, dtype=torch.float32, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        recon, _, _ = vae(Xt)\n",
    "        err = torch.mean(torch.abs(recon - Xt), dim=1)  # MAE per sample\n",
    "    return err.cpu().numpy()\n",
    "\n",
    "val_scores_vae = recon_error(X_val_p)\n",
    "test_scores_vae = recon_error(X_test_p)\n",
    "\n",
    "# =========================================================\n",
    "# 7) TRAIN 3 MODELS ON EXTRACTED FEATURES (NORMAL ONLY)\n",
    "# =========================================================\n",
    "iso = IsolationForest(n_estimators=400, contamination=\"auto\", random_state=SEED, n_jobs=-1)\n",
    "ocsvm = OneClassSVM(kernel=\"rbf\", nu=0.05, gamma=\"scale\")\n",
    "lof = LocalOutlierFactor(n_neighbors=35, novelty=True, metric=\"minkowski\")\n",
    "\n",
    "iso.fit(Z_train_norm)\n",
    "ocsvm.fit(Z_train_norm)\n",
    "lof.fit(Z_train_norm)\n",
    "\n",
    "def score_model(model, Z):\n",
    "    return -model.decision_function(Z)  # higher => more anomalous\n",
    "\n",
    "# =========================================================\n",
    "# 8) EVALUATION (threshold tuned on VAL by F1)\n",
    "# =========================================================\n",
    "results = []\n",
    "\n",
    "# ---- VAE reconstruction (not one of the 3 models, but useful baseline)\n",
    "best_vae = tune_threshold_by_f1(y_val, val_scores_vae)\n",
    "m_vae, cm_vae, rep_vae = evaluate_from_scores(y_test, test_scores_vae, best_vae[\"threshold\"], \"VAE_ReconError\")\n",
    "print_metrics_block(\"VAE Reconstruction Error\", m_vae, cm_vae, rep_vae)\n",
    "results.append(m_vae)\n",
    "\n",
    "# ---- IsolationForest on VAE features\n",
    "val_scores_iso = score_model(iso, Z_val)\n",
    "test_scores_iso = score_model(iso, Z_test)\n",
    "best_iso = tune_threshold_by_f1(y_val, val_scores_iso)\n",
    "m_iso, cm_iso, rep_iso = evaluate_from_scores(y_test, test_scores_iso, best_iso[\"threshold\"], \"IsolationForest_on_VAE\")\n",
    "print_metrics_block(\"IsolationForest on VAE Features\", m_iso, cm_iso, rep_iso)\n",
    "results.append(m_iso)\n",
    "\n",
    "# ---- OneClassSVM on VAE features\n",
    "val_scores_svm = score_model(ocsvm, Z_val)\n",
    "test_scores_svm = score_model(ocsvm, Z_test)\n",
    "best_svm = tune_threshold_by_f1(y_val, val_scores_svm)\n",
    "m_svm, cm_svm, rep_svm = evaluate_from_scores(y_test, test_scores_svm, best_svm[\"threshold\"], \"OneClassSVM_on_VAE\")\n",
    "print_metrics_block(\"OneClassSVM on VAE Features\", m_svm, cm_svm, rep_svm)\n",
    "results.append(m_svm)\n",
    "\n",
    "# ---- LOF on VAE features\n",
    "val_scores_lof = score_model(lof, Z_val)\n",
    "test_scores_lof = score_model(lof, Z_test)\n",
    "best_lof = tune_threshold_by_f1(y_val, val_scores_lof)\n",
    "m_lof, cm_lof, rep_lof = evaluate_from_scores(y_test, test_scores_lof, best_lof[\"threshold\"], \"LOF_on_VAE\")\n",
    "print_metrics_block(\"LOF on VAE Features\", m_lof, cm_lof, rep_lof)\n",
    "results.append(m_lof)\n",
    "\n",
    "# =========================================================\n",
    "# 9) SUMMARY TABLE\n",
    "# =========================================================\n",
    "summary = pd.DataFrame(results)\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"summary_metrics.csv\")\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "print(\"\\n✅ Saved summary:\", os.path.abspath(summary_path))\n",
    "print(summary[[\"model\", \"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\", \"pr_auc\", \"threshold\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f103af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted latent features: ['z_mu_0', 'z_mu_1', 'z_mu_2', 'z_mu_3', 'z_mu_4', 'z_mu_5', 'z_mu_6', 'z_mu_7', 'z_mu_8', 'z_mu_9', 'z_mu_10', 'z_mu_11', 'z_mu_12', 'z_mu_13', 'z_mu_14', 'z_mu_15', 'z_mu_16', 'z_mu_17', 'z_mu_18', 'z_mu_19', 'z_mu_20', 'z_mu_21', 'z_mu_22', 'z_mu_23', 'z_mu_24', 'z_mu_25', 'z_mu_26', 'z_mu_27', 'z_mu_28', 'z_mu_29', 'z_mu_30', 'z_mu_31']\n",
      "Latent feature matrix shape: (82400, 32)\n",
      "     z_mu_0    z_mu_1    z_mu_2    z_mu_3    z_mu_4    z_mu_5    z_mu_6  \\\n",
      "0 -0.906946 -0.189529  0.002966  0.041748  1.001947 -0.021322  0.045265   \n",
      "1 -0.431586 -0.207037 -0.033935  0.040579 -0.030113 -0.045382  0.016980   \n",
      "2 -0.803885 -0.214530 -0.018176  0.027154 -0.263716 -0.023137  0.030048   \n",
      "3  0.525172 -0.142282 -0.043666  0.037461  0.198523 -0.026219  0.045868   \n",
      "4  0.913633  0.127134  0.012624 -0.048581 -0.360730  0.015022 -0.036639   \n",
      "\n",
      "     z_mu_7    z_mu_8    z_mu_9  ...   z_mu_22   z_mu_23   z_mu_24   z_mu_25  \\\n",
      "0  0.038647  0.008978 -0.023701  ...  0.021850 -0.019756 -0.030129 -0.066949   \n",
      "1  0.080743 -0.059829 -0.041770  ...  0.087898 -0.010448  0.030096 -0.119255   \n",
      "2  0.060388 -0.035354 -0.014466  ...  0.049914 -0.015997 -0.009718 -0.104331   \n",
      "3  0.063638 -0.027630  0.010295  ...  0.056992  0.025138  0.003537 -0.093520   \n",
      "4  0.004301  0.019696  0.002376  ... -0.029019  0.003249  0.021895  0.071943   \n",
      "\n",
      "    z_mu_26   z_mu_27   z_mu_28   z_mu_29   z_mu_30   z_mu_31  \n",
      "0  0.021447  0.045976  0.245207  0.047391 -0.027773 -0.309446  \n",
      "1  0.034259  0.068106  0.288137  0.047055 -0.023170  1.321473  \n",
      "2  0.024322  0.040865  0.259254  0.058883 -0.018501  0.168255  \n",
      "3  0.010957  0.037304  0.241112  0.027013 -0.035805 -0.660707  \n",
      "4 -0.029721 -0.007073 -0.190027 -0.006955  0.018132  0.578112  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "latent_dim = Z_train_full.shape[1]\n",
    "latent_feature_names = [f\"z_mu_{i}\" for i in range(latent_dim)]\n",
    "\n",
    "print(\"Extracted latent features:\", latent_feature_names)\n",
    "print(\"Latent feature matrix shape:\", Z_train_full.shape)\n",
    "latent_df = pd.DataFrame(Z_train_full, columns=latent_feature_names)\n",
    "print(latent_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c452b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "import joblib\n",
    "model_filename = 'trained_model.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps1 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
